{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a8c14-0377-4135-b6c2-bcc3215812e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#single head of self-attention used in multi-head attention mechanisms within transformer models\n",
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        #key is the info that you want to compare against or use as a reference\n",
    "        self.key=nn.Linear(n_embd,head_size,bias=False)\n",
    "\n",
    "        #query is the info that you currently processing or seeking ti understand better\n",
    "        self.query=nn.Linear(n_embd,head_size,bias=False)\n",
    "\n",
    "        #value is the info tassociated with query which provides additional context \n",
    "        self.value=nn.Linear(n_embd,head_size,bias=False)\n",
    "\n",
    "        #buffer contains a lower triangular matrix with ones below the main diagonal and zeros above it.\n",
    "        #This matrix is used for masking during attention computations.\n",
    "        self.register_buffer('trill',torch.tril(torch.ones(block_size,block_size)))\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    #forward implements the self-attention mechanism for one attention head. \n",
    "    def forward(self,x):\n",
    "\n",
    "        #B, T, and C represent the batch size, sequence length, and input dimension\n",
    "        B,T,C=x.shape\n",
    "        \n",
    "        k=self.key(x)\n",
    "        q=self.query(x)\n",
    "\n",
    "        #The attention mechanism computes the attention weights using the dot product between query and key vectors.\n",
    "        # which is scaled by a factor of k.shape[-1]**-0.5 (the square root of the key dimension).\n",
    "        wei= q @ k.transpose(-2,-1)* k.shape[-1]**-0.5\n",
    "\n",
    "        #his masking ensures that the model doesn't attend to future elements in the sequence\n",
    "        wei=wei.masked_fill(self.trill[:T,:T] ==0, float('-inf'))\n",
    "\n",
    "        #normalize them and obtain valid attention probabilities.\n",
    "        wei=F.softmax(wei,dim=-1)\n",
    "        wei=self.dropout(wei)\n",
    "        \n",
    "        v=self.value(x)\n",
    "        out=wei @ v\n",
    "        return out \n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"mulitple heads of self - attention in parallel\"\"\"\n",
    "    \n",
    "    def __init__(self,num_heads,head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        #num_heads represents the number of attention heads to use in parallel.\n",
    "        #head_size is the number of features captured by each attention head.\n",
    "\n",
    "        # a container for multiple attention heads.\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "        #self.prj is a linear projection layer used to combine the outputs of the individual attention heads.\n",
    "        self.proj=nn.Linear(head_size*num_heads,n_embd)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        #outputs of the attention heads are concatenated along the last dimensio\n",
    "        out=torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "        out=self.dropout(self.proj(out))\n",
    "        return out\n",
    "        \n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple layer  followed by a non-linearlity \"\"\"\n",
    "    def __init__(self,imbd):\n",
    "        super().__init__()\n",
    "\n",
    "        # a container for defining a sequence of operations in PyTorch.\n",
    "        self.net=nn.Sequential(\n",
    "            \n",
    "            # takes an input of dimension n_embd and produces an intermediate output with a dimension that is four times the input dimension\n",
    "            nn.Linear(n_embd,4*n_embd),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # reduces the dimensionality back to the original input dimension\n",
    "            nn.Linear(4*n_embd,n_embd),\n",
    "\n",
    "            #The dropout layer  is used for regularization\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        def forward(self,x):\n",
    "            return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__ (self,n_embd,n_head) :\n",
    "        super().__init__()\n",
    "        #head size is the number of features that each head will be capturing in our multi-head attention\n",
    "        head_size=n_embd // n_head\n",
    "\n",
    "        #self attention \n",
    "        self.sa=MultiHeadAttention(n_head,head_size)\n",
    "\n",
    "        #The feedforward layer is responsible for capturing complex patterns and features in the data.\n",
    "        self.ffwd=FeedFoward(n_embd)\n",
    "\n",
    "        #Layer normalization helps stabilize training by normalizing the activations within each laye\n",
    "        self.ln1=nn.LayerNorm(n_embd)\n",
    "        self.ln2=nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y=self.sa(x)\n",
    "        x=self.ln1(x+y)\n",
    "        y=self.ffwd(x)\n",
    "        x=self.ln2(x+y)\n",
    "        return x\n",
    "        \n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__ (self,vocab_size) :\n",
    "        #This line calls the constructor of the parent class, which is typically necessary when defining a subclass in Python\n",
    "        super().__init__()\n",
    "\n",
    "        #This line creates an embedding layer for token embeddings.\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size,n_embd)\n",
    "\n",
    "        #This line creates an embedding layer for positional embeddings.\n",
    "        self.postion_embedding_table=nn.Embedding(block_size,n_embd)\n",
    "\n",
    "        #This line defines a sequence of neural network blocks.\n",
    "        self.blocks=nn.Sequential(*[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n",
    "\n",
    "        #This line defines a layer normalization operation. \n",
    "        #Layer normalization is used to stabilize and normalize the activations between layers in a neural network.\n",
    "        self.ln_f=nn.LayerNorm(n_embd)\n",
    "\n",
    "        #This line defines a linear (fully connected) layer for language modeling.\n",
    "        self.lm_head=nn.Linear(n_embd,vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self,module):\n",
    "        \n",
    "        if isinstance(module,nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module,nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "    \n",
    "            \n",
    "    # forward method for a neural network model, which is used for making predictions and computing losses during training\n",
    "    def forward(self,index,targets=None):\n",
    "\n",
    "         # takes the input index, which likely represents a sequence of token indices, and uses the self.token_embedding_table to obtain token embeddings\n",
    "        logits=self.token_embedding_table(index)\n",
    "\n",
    "        #.shape is used the unpack the items of logits  into B ,T , C\n",
    "        # B is for batch, T is for time ,C is for number of class\n",
    "        B, T, C = logits.shape\n",
    "        \n",
    "        #idx and tarbets are both (B,T) tensor of integers\n",
    "        tok_emb=self.token_embedding_table(index) \n",
    "        pos_emb=self.postion_embedding_table(torch.arange(T,device=device))\n",
    "\n",
    "        # This line combines the token embeddings and positional embeddings by element-wise addition\n",
    "        x=tok_emb+pos_emb\n",
    "        x=self.blocks(x)\n",
    "        x.self.ln_f(x)\n",
    "        logits=self.lm_head(x)\n",
    "        \n",
    "        if targets==None:\n",
    "            loss=None\n",
    "        else :\n",
    "           \n",
    "            #.view is used to pack them alternate of .shape\n",
    "            logits=logits.view(B*T,C)\n",
    "            targets=targets.view(B*T)\n",
    "            \n",
    "            #This function computes the loss between the predicted logits (logits) and the ground truth labels (targets).\n",
    "            loss=F.cross_entropy(logits,targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    #  purpose of generate func -generate a sequence of tokens or indices given an initial context (index)\n",
    "    #  and a maximum number of new tokens (max_new_tokens).\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        # Create a new tensor for the generated sequence\n",
    "        generated_sequence = index\n",
    "    \n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(generated_sequence)\n",
    "            \n",
    "            #focus only on the last time step\n",
    "            logits=logits[:,-1,:]\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            num_samples=1\n",
    "            index_next = torch.multinomial(probs, num_samples)\n",
    "    \n",
    "            # append sampled index to the running sequence\n",
    "            generated_sequence = torch.cat((generated_sequence, index_next), dim=1)\n",
    "    \n",
    "        return generated_sequence\n",
    "# the \"Forward\" function makes predictions based on input, and the \"Generate\" function uses those predictions to create new text,\n",
    "# like continuing a story or generating sentences.\n",
    "\n",
    "#we are creating an instance of BigramLanguageModel name model\n",
    "model=GPTLanguageModel(vocab_size)\n",
    "\n",
    "#m is the alternate verson of model in but running in gpu(if available)\n",
    "m = model.to(device)\n",
    "\n",
    "#we initially declaring a context of 1-dim zero as our starting chars\n",
    "context=torch.ones((1,1),dtype=torch.long,device=device)\n",
    "\n",
    "#generated_chars is using class for predicting the next words for context and predticting upto 500 words \n",
    "generated_chars=decode(m.generate(context,max_new_tokens=50)[0].tolist())\n",
    "print(generated_chars)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
